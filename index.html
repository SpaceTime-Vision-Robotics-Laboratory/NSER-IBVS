<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Efficient Self-Supervised Neuro-Analytic Visual Servoing for Real-time
        Quadrotor Control</title>
    <link rel="stylesheet" href="style.css">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-90M4DDNEBC"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-90M4DDNEBC');
    </script>
</head>
<body>
    <div class="title">
        <h1>Efficient Self-Supervised Neuro-Analytic </h1>
        <h2>Visual Servoing for Real-time
            Quadrotor Control</h2>
        <div class="conference" style="color: #555;">ICCV 2025 - The 21st Embedded Vision Workshop</div>
    </div>

    <div class="authors">
        <a href="https://scholar.google.com/citations?user=osyBED4AAAAJ&hl=en" target="_blank" class="author" style="color: #0066cc;">Sebastian Mocanu<sup>1</sup></a>
        <a href="https://scholar.google.com/citations?user=VBogtPAAAAAJ&hl=en" target="_blank" class="author" style="color: #0066cc;">Sebastian-Ion Nae<sup>1</sup></a>
        <a href="https://scholar.google.com/citations?user=9GlsUHAAAAAJ&hl=en&oi=ao" target="_blank" class="author" style="color: #0066cc;">Mihai Eugen Barbu<sup>1</sup></a>
        <a href="https://scholar.google.com/citations?user=se9kni0AAAAJ&hl=en" target="_blank" class="author" style="color: #0066cc;">Marius Leordeanu<sup>1,2,3</sup></a>
    </div>

    <div class="affiliations">
        <sup>1</sup>National University of Science and Technology POLITEHNICA Bucharest, Romania<br>
        <sup>2</sup>Institute of Mathematics "Simion Stoilow" of the Romanian Academy, Romania<br>
        <sup>3</sup>NORCE Norwegian Research Center, Norway
    </div>

    <div class="links">
        <a href="assets/EVW-10_Efficient_Self_Supervised_Neuro_Analytic_Visual_Servoing_for_Real_time_Quadrotor_Control-10.pdf" target="_blank">Paper</a>
        <a href="assets/poster.pdf" target="_blank">Poster</a>
        <a href="https://github.com/SheepSeb/Vision-Language-UAV-Following-Control" target="_blank">Code</a>
    </div>

    <!-- TODO Here also add multiple small videos that auto replay -->
    <!-- TODO Fix the auto loading and reloading of this video. Do the same with the other "mini" demos -->
    <div class="section">
        <h2>Overview Video</h2>
        <div class="video-container">
            <div class="video-wrapper">
                <iframe 
                    id="video-iframe"
                    src="assets/Drone-Car-ICCV-EVW-x3-Speed.mp4" 
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen
                    autoplay
                    muted>
                </iframe>
            </div>
            <p>Demo comparing the teacher and the student methods showcasing examples where the teacher is faster, where the student is faster and when the teacher fails to do the task.</p>
        </div>
    </div>

    <div class="section">
        <h2>Motivation and Contribution</h2>
        <div class="abstract">
            <p><strong>Challenge: </strong>Classical IBVS methods suffer from numerical instabilities and singularities, while marker-based approaches (ArUco, AprilTags) limit deployment in dynamic indoor environments. GPS-denied scenarios demand efficient, marker-free visual servoing for quadrotor control.</p>
            <p><strong>Our Solution: </strong>We present a self-supervised neuro-analytical framework featuring a <strong><u>N</u></strong>umerically <strong><u>S</u></strong>table <strong><u>E</u></strong>fficient and <strong><u>R</u></strong>educed (NSER) Image-Based Visual Servoing (IBVS)  teacher model, distilled into a lightweight 1.7M parameter student network achieving <strong>11x</strong> real-time performance with <strong>improved control accuracy</strong>.</p>
        </div>
        
        <div class="key-contributions">
            <h3>Key Contributions</h3>
            <ul>
                <li><strong>Stable analytical teacher: </strong>Improved IBVS controller solving numerical instabilities through reduced classical equations, enabling robust marker-free control.</li>
                <li><strong>Two-stage segmentation: </strong>YOLOv11 + U-Net mask splitter for anterior-posterior vehicle segmentation, accurately estimating target orientation.</li>
                <li><strong>Efficient knowledge distillation: </strong>Dual-path system transferring geometric visual servoing from teacher to compact student neural network that outperforms the teacher while suitable for onboard deployment.</li>
                <li><strong>Practical sim-to-real transfer: </strong>Digital-twin training with real-world fine-tuning, validated in GPS-denied indoor environments with minimal hardware.</li>
            </ul>
        </div>
    </div>

    <div class="section">
        <h2>Our Approach</h2>
    </div>


    <div class="section">
        <h2>Experimental Setup</h2>
    </div>

    <div class="section">
        <h2>Numeric Results</h2>

        <h3>Performance comparison for both methods on real-world and digital-twin environments</h3>
        
        <h3>Inference Time Analysis</h3>

    </div>


    <div class="section">
        <h2>Visual Results</h2>

        <h3>Control command and error evolutions over time</h3>
        
        <h3>Trajectory Analysis</h3>
    </div>



    <div class="section">
        <h2>Detection Results & Samples</h2>
        
        <p>Our vision-language-guided UAV tracking framework demonstrates robust performance across diverse indoor scenarios. We collected 14,400 high-resolution frames (3840 × 2160) using automated pseudo-labeling with 98.6% accuracy.</p>
        
        <div class="large-results">
            <div class="large-result-item">
                <img src="imgs/image_edits/multiple_images.png" alt="Multiple Target Detection Examples">
                <p><strong>Multi-Target Following:</strong> UAVs tracking various targets including ground vehicles and aerial drones across indoor classrooms, constrained spaces, and diverse conditions.</p>
            </div>
            <div class="large-result-item">
                <img src="imgs/image_edits/miss_detection.png" alt="Detection Success and Failure Cases">
                <p><strong>Detection Accuracy:</strong> Success cases (left, right) and failure case (center) showing 98.6% automated labeling accuracy with only 1.4% requiring manual correction.</p>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Dataset Comparison</h2>
        <p>Our indoor drone dataset maintains comparable size to datasets used for YOLOv8 fine-tuning, with the key distinction being the temporal information preserved in our video-based dataset.</p>
        
        <table>
            <thead>
                <tr>
                    <th>Paper</th>
                    <th>Dataset</th>
                    <th>No. of Videos</th>
                    <th>No. of Images</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Ours</strong></td>
                    <td>Ours (only drone footage)</td>
                    <td>4</td>
                    <td>14,400</td>
                </tr>
                <tr>
                    <td>Real-Time Flying OD with YOLOv8</td>
                    <td>Dataset 1</td>
                    <td>No videos</td>
                    <td>15,064</td>
                </tr>
                <tr>
                    <td>Real-Time Flying OD with YOLOv8</td>
                    <td>Dataset 2</td>
                    <td>No videos</td>
                    <td>11,998</td>
                </tr>
                <tr>
                    <td>Dogfight</td>
                    <td>NPS-Drones</td>
                    <td>14</td>
                    <td>70,250</td>
                </tr>
                <tr>
                    <td>Dogfight</td>
                    <td>FL-Drones</td>
                    <td>50</td>
                    <td>38,948</td>
                </tr>
            </tbody>
        </table>
        <div class="table-caption">Table 1: Dataset comparison showing our indoor drone dataset maintains comparable size to datasets used for YOLOv8 fine-tuning. While smaller than Dogfight datasets, our high-quality annotations address potential labeling inconsistencies present in larger datasets.</div>
    </div>

    <div class="section">
        <h2>GradCAM Attention Analysis</h2>
        <p>To validate detection reliability, we employed GradCAM to visualize YOLOv8's attention patterns across different scenarios. This analysis reveals important insights about the model's behavior in various environments.</p>
        
        <div class="two-column">
            <div class="result-item">
                <img src="imgs/gradCAM/gradCAM.png" alt="GradCAM Human-biased Attention">
                <p><strong>Mixed Human-Drone Scenarios:</strong> The model shows strong bias toward human subjects, potentially compromising drone detection in human-populated environments.</p>
            </div>
            <div class="result-item">
                <img src="imgs/gradCAM/gradCAM2.png" alt="GradCAM Drone-focused Attention">
                <p><strong>Drone-only Scenarios:</strong> Distributed attention across multiple UAV targets with reduced environmental interference, indicating robust multi-target tracking capabilities.</p>
            </div>
        </div>
        
        <div class="abstract" style="margin-top: 30px;">
            <p><strong>Implications for Deployment:</strong> The observed attention bias suggests potential performance degradation in human-populated environments. This validates our dataset design focusing on indoor drone-to-drone scenarios where human interference is minimized. The distributed attention pattern in drone-centric scenarios indicates robust multi-target tracking capabilities essential for autonomous operation.</p>
        </div>
    </div>

    <div class="section">
        <h2>YOLOv8 Performance Evaluation</h2>
        <p>We assess model performance across precision, recall, mAP50-95, and inference latency to highlight trade-offs between accuracy and real-time feasibility. YOLOv8-nano trained for 25 epochs achieves the best balance: 0.97 precision, 0.97 recall, 0.79 mAP50-95, and a low 4.7 ms inference time.</p>
        
        <table>
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Epochs</th>
                    <th>Precision ↑</th>
                    <th>Recall ↑</th>
                    <th>mAP50-95 ↑</th>
                    <th>Inference Time ↓ (ms)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td rowspan="3">YOLOv8-nano</td>
                    <td>5</td>
                    <td>0.95</td>
                    <td>0.92</td>
                    <td>0.67</td>
                    <td>4.7</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td>0.97</td>
                    <td>0.97</td>
                    <td>0.72</td>
                    <td>4.7</td>
                </tr>
                <tr>
                    <td>25</td>
                    <td>0.97</td>
                    <td><strong>0.97</strong></td>
                    <td><strong>0.79</strong></td>
                    <td><strong>4.7</strong></td>
                </tr>
                <tr>
                    <td rowspan="3">YOLOv8-small</td>
                    <td>5</td>
                    <td>0.93</td>
                    <td>0.93</td>
                    <td>0.62</td>
                    <td>6.9</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td>0.95</td>
                    <td>0.94</td>
                    <td>0.72</td>
                    <td>7.0</td>
                </tr>
                <tr>
                    <td>25</td>
                    <td><strong>0.98</strong></td>
                    <td><strong>0.97</strong></td>
                    <td>0.78</td>
                    <td>7.0</td>
                </tr>
                <tr>
                    <td rowspan="3">YOLOv8-large</td>
                    <td>5</td>
                    <td>0.86</td>
                    <td>0.83</td>
                    <td>0.56</td>
                    <td>29.6</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td>0.94</td>
                    <td>0.93</td>
                    <td>0.69</td>
                    <td>32.4</td>
                </tr>
                <tr>
                    <td>25</td>
                    <td>0.97</td>
                    <td>0.95</td>
                    <td>0.77</td>
                    <td>31.3</td>
                </tr>
            </tbody>
        </table>
        <div class="table-caption">Table 2: Comparative evaluation of YOLOv8 architectures across training epochs, demonstrating the precision-efficiency trade-off critical for onboard deployment. Bold values indicate selected configuration optimizing for real-time constraints.</div>
    </div>

    <div class="section">
        <h2>Control Algorithm Comparison</h2>
        
        <p>We compare our depth-free IBVS controller against traditional PID control for indoor UAV tracking. IBVS achieves superior centering performance (89-92% vs 68-71%) while operating solely on 2D visual features.</p>
        
        <h3>Algorithm Architectures</h3>
        <div class="two-column">
            <div class="result-item">
                <img src="imgs/control/control_algorithm_pid.png" alt="PID Control Algorithm">
                <p><strong>PID Controller:</strong> Traditional position-based control approach</p>
            </div>
            <div class="result-item">
                <img src="imgs/control/schematic-ibvs-tracking.png" alt="IBVS Control Schematic">
                <p><strong>IBVS Controller:</strong> Vision-based servoing control architecture</p>
            </div>
        </div>

        <h3>Spatial Distribution Analysis</h3>
        <div class="three-column">
            <div class="result-item">
                <img src="imgs/control/ibvs-line/03_detection_heatmap.png" alt="IBVS Line Heatmap">
                <p>IBVS Line Trajectory - Detection Heatmap</p>
            </div>
            <div class="result-item">
                <img src="imgs/control/ibvs-round/03_detection_heatmap.png" alt="IBVS Round Heatmap">
                <p>IBVS Circular Trajectory - Detection Heatmap</p>
            </div>
            <div class="result-item">
                <img src="imgs/control/tracking-pid-line/03_detection_heatmap.png" alt="PID Line Heatmap">
                <p>PID Line Trajectory - Detection Heatmap</p>
            </div>
        </div>

        <h3>Controller Performance Comparison</h3>
        <p>IBVS achieves superior tracking with 89% (straight-line) and 92% (circular) centering scores versus PID's 68% and 71%. IBVS demonstrates higher control authority but increased command variability.</p>
        
        <table>
            <thead>
                <tr>
                    <th>Scenario</th>
                    <th>Controller</th>
                    <th>Range vx/vy/ωz Cmd (m/s)</th>
                    <th>Centering Score</th>
                    <th>Smoothness</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td rowspan="2">Straight Line</td>
                    <td>PID</td>
                    <td>[0, 0]/[-10, 30]/[-4, 4]</td>
                    <td>0.68</td>
                    <td>High</td>
                </tr>
                <tr>
                    <td><strong>IBVS</strong></td>
                    <td>[-4, 4]/[-10, 40]/[0, 25]</td>
                    <td><strong>0.89</strong></td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td rowspan="2">Circular</td>
                    <td>PID</td>
                    <td>[0, 0]/[-10, 30]/[-5, 15]</td>
                    <td>0.71</td>
                    <td>Low</td>
                </tr>
                <tr>
                    <td><strong>IBVS</strong></td>
                    <td>[-10, 20]/[-20, 30]/[0, 9]</td>
                    <td><strong>0.92</strong></td>
                    <td>High</td>
                </tr>
            </tbody>
        </table>
        <div class="table-caption">Table 3: Controller performance comparison. Centering Score represents the fraction of time the target remained within the central 30% of the image frame. Command Smoothness is qualitatively assessed from standard deviation analysis.</div>

        <p>The depth-decoupled IBVS approach shows clear advantages in tracking accuracy and adaptability to complex trajectories. IBVS exhibits particularly improved performance during high-curvature segments where PID shows erratic behavior.</p>
    </div>

    <div class="section">
        <h2>Conclusion</h2>
        <p>We presented a self-supervised framework for autonomous indoor UAV tracking that combines scalable dataset generation with depth-free visual servoing control. Leveraging a frozen vision-language model (GroundingSAM), we achieved 98.6% labeling accuracy across a high-resolution indoor drone dataset, significantly reducing manual annotation effort.</p>
        
        <p>Our evaluation of YOLOv8 variants highlights the trade-off between accuracy and inference latency. YOLOv8-nano strikes the best balance for real-time deployment (0.97 precision/recall at 4.7 ms), while larger models offer marginal gains at a cost to onboard performance. The proposed depth-decoupled IBVS controller enables stable target following using only 2D visual features, improving centering performance (89–92% vs. 68–71%) compared to PID baseline, especially in complex trajectories.</p>
        
        <p>Despite some limitations including fixed flight altitude assumptions and detection reliability degradation under occlusion or low contrast, our approach demonstrates a reproducible and scalable pipeline for markerless indoor UAV tracking with potential applications in warehouse automation, surveillance, and multi-agent indoor robotics.</p>
    </div>

    <div class="section">
        <h2>Resources</h2>
        <p>
            <a href="https://github.com/SheepSeb/Vision-Language-UAV-Following-Control">📁 GitHub Repository</a> |
            <a href="https://youtu.be/67IEGqvVRYI">🎥 Demo Video</a> |
            <a href="assets/BRAIT_2025_final.pdf">📑 Paper (PDF)</a> |
            <a href="#contact">✉️ Contact</a>
        </p>
        
        <div id="contact" style="margin-top: 30px;">
            <h3>Contact</h3>
            <p>For questions and collaboration inquiries, please contact the authors through the GitHub repository or academic channels.</p>
        </div>
    </div>

    <div class="section">
        <h2>Citation</h2>
        <div class="citation">
@article{mocanu2025efficient,
  title   = {Efficient Self-Supervised Neuro-Analytic Visual Servoing for Real-time Quadrotor Control},
  author  = {Sebastian Mocanu, Sebastian-Ion Nae, Mihai-Eugen Barbu, and Marius Leordeanu},
  journal = {International Conference on Computer Vision Workshops 2025 (ICCVW 2025)},
  year    = {2025}
}
</div>
    <button id="copy-citation-button" class="citation-button">Copy Citation</button>

    </div>

    <footer style="text-align: center; margin-top: 60px; padding: 20px; color: #666; border-top: 1px solid #ddd;">
        <p>&copy; 2025 Sebastian Mocanu, Sebastian-Ion Nae, Mihai Eugen Barbu and Marius Leordeanu</p>
        <p style="font-size: 0.9em; margin-top: 10px;">
            <a href="https://github.com/SheepSeb/Vision-Language-UAV-Following-Control">View source on Github</a>
        </p>
    </footer>

    <script src="app.js" type="text/javascript"></script>
</body>
</html>
